
# 강화학습 기반 유저 적응형 협동 전투 NPC – 발표 대본 v2

> ⚠️ 기준: **20분 발표**를 상정하고 작성.  
> 슬라이드 번호는 네 PPT 기준으로 맞춰서 구성했어.

---

## 슬라이드 1 – 제목

**강화학습 기반 유저 적응형 협동 전투 NPC**

> 안녕하세요, 손승현, 박준형입니다.  
> 저희가 발표할 주제는 **강화학습 기반 유저 적응형 협동 전투 NPC 시스템**입니다.  
> 한 줄로 말하면, *플레이어의 실력과 스타일에 맞춰 협동 방식이 달라지는 동료 NPC*를  
> 강화학습으로 만드는 연구입니다.

---

## 슬라이드 2 – 문제 정의 & 목표

**키워드: 규칙 기반 AI 한계 / 난이도 조절 / 실시간 적응 불가**

> 먼저 왜 이런 연구를 했는지부터 말씀드리겠습니다.  
> 
> 기존 게임의 규칙 기반 AI, 예를 들어 비헤이비어 트리나 FSM 같은 방식은  
> 행동 패턴이 어느 정도 정해져 있어서, 플레이어가 금방 패턴을 외우게 됩니다.  
> 그러면 AI 행동이 예측 가능해지고, 결국 **몰입감이 떨어진다**는 문제가 있습니다.  
> 
> 또 난이도 조절 측면에서도, 유저 실력이 다양해질수록  
> 디자이너가 규칙을 일일이 손으로 튜닝해야 해서 **유지보수 비용이 커지고**,  
> 플레이어 실력이 바뀌어도 **실시간으로 적응하기 어렵다**는 한계가 있습니다.  
> 
> 그래서 저희 목표는, 플레이어의 숙련도를 반영해서  
> - 초보자에게는 전투를 적극적으로 이끌고 보호해주는 **리더형 NPC**,  
> - 숙련자에게는 플레이어를 보조하고 시너지를 맞춰주는 **서포터형 NPC**  
> 를 **강화학습으로 자동 학습시키는 파이프라인**을 만드는 것입니다.

---

## 슬라이드 3 – 제안 방법: 2단계 학습 파이프라인

**키워드: Step 1 Self-Play / Step 2 협동 학습 / Player Models**

> 제안 방법은 크게 두 단계 파이프라인입니다.  
> 
> **1단계는 Self-Play 학습**입니다.  
> 같은 정책을 공유하는 에이전트들끼리 5 대 5로 싸우게 하고,  
> PPO와 파라미터 공유 방식을 사용해서  
> 최종적으로 **양 팀 승률이 50:50에 가까워지는 균형 상태**를 목표로 학습합니다.  
> 이 과정에서 특정 에피소드마다 정책 체크포인트를 저장해 두고,  
> 이를 이후에 **초보 / 중급 / 숙련 플레이어 모델**로 재사용합니다.  
> 
> **2단계는 협동 NPC 학습**입니다.  
> 1단계에서 얻은 체크포인트 중 일부를 고정해서 *플레이어 역할*로 두고,  
> 그 옆에서 함께 싸우는 NPC만 따로 학습시킵니다.  
> 이때는 승패 보상뿐만 아니라, 플레이어와의 거리, 협공 여부 등을 반영한  
> **협동 보상**을 추가해서, 각 숙련도에 최적인 협동 패턴을 학습하도록 설계했습니다.  
> 
> 정리하면, 1단계에서 **숙련도별 플레이어 모델을 만들고**,  
> 2단계에서 **그 모델과 협동하는 NPC를 학습**하는 구조입니다.

---

## 슬라이드 4 – 전투 환경 설계

**키워드: 20×20 격자 / 5vs5 / 지형 요소**

> 환경은 **20×20 격자 맵**입니다.  
> 양 팀은 각각 5명으로 구성된 5 대 5 전투를 하고,  
> 한 에피소드는 최대 200 스텝까지 진행됩니다.  
> 
> 맵에는 벽 타일, 위험 타일, 버프 타일이 일정 비율로 섞여 있어서  
> 단순히 가까운 적만 따라가는 수준이 아니라,  
> **지형을 고려한 포지셔닝과 이동 전략**이 필요하게 설계했습니다.  
> 
> 이렇게 격자 환경으로 단순화한 이유는,  
> 초기 단계에서는 **전투 협동 패턴 자체를 검증**하는 것이 목적이기 때문입니다.  
> 3D 연속 공간까지 확장하는 부분은 한계와 향후 연구에서 다시 말씀드리겠습니다.

---

## 슬라이드 5 – 역할 및 능력치 설계

**키워드: 탱커 / 딜러 / 힐러 / 레인저 / 서포터**

> 각 팀은 탱커, 딜러, 힐러, 레인저, 서포터로 구성됩니다.  
> 
> - **탱커**는 HP와 방어력이 높고, '도발' 스킬을 사용해서  
>   적의 공격을 자신에게 끌어오는 역할을 합니다.  
> - **딜러**는 공격력이 높고, 여러 적에게 피해를 주는 '범위 공격' 스킬을 가집니다.  
> - **힐러**는 회복 스킬을 통해 아군의 HP를 복구시킵니다.  
> - **레인저**는 사거리가 길고, 직선상 여러 적을 관통하는 '관통샷'을 사용합니다.  
> - **서포터**는 주변 아군에게 버프를 주는 스킬을 가집니다.  
> 
> 이렇게 역할과 스킬을 분리한 이유는, 실제 게임에서처럼  
> **직업 간 시너지와 협동 구조**가 강화학습 보상 설계에 자연스럽게 반영되도록 하기 위함입니다.

---

## 슬라이드 6 – 상태 공간(229차원) 설계

**키워드: 자기 상태 / 아군·적군 상태 / 주변 지형 / 플레이어 정보**

> 에이전트가 보는 상태는 총 **229차원 벡터**입니다.  
> 초등학생 버전으로 말하면, “AI에게 상황을 설명하는 **질문 229개짜리 설문지**”라고 볼 수 있습니다.  
> 
> 구성은 다음과 같습니다.
> 
> 1. **자기 자신 정보**  
>    - HP, MP, 위치, 방향, 살아 있는지 여부, 역할 one-hot 등  
> 2. **나머지 아군 4명 정보**  
> 3. **적군 5명 정보**  
>    → 각 유닛마다 비슷한 형태의 정보가 들어갑니다.  
> 4. **지형 정보**  
>    - 자기 기준 11×11 영역에 대해,  
>      각 칸이 벽인지, 위험인지, 버프인지 등을 인코딩한 값들입니다.  
> 5. **전역 정보 및 플레이어 정보**  
>    - 현재 턴 수, 팀 ID,  
>    - 2단계 협동 학습에서는 플레이어의 위치와 상대 거리 등이 추가됩니다.  
> 
> 이렇게 해서 한 에이전트가 “나, 우리 팀, 적 팀, 주변 지형, 플레이어”까지  
> 한 번에 고려할 수 있도록 **상태 공간을 229차원**으로 설계했습니다.

---

## 슬라이드 7 – 행동 공간: 12개 이산 행동

**키워드: 우리가 정의한 12개 메뉴 / Softmax로 확률 뽑기**

> 행동 공간은 **12개의 이산 행동**으로 구성했습니다.  
> 중요한 점은, 이 12개는 **신경망이 스스로 만드는 게 아니라,  
> 저희가 환경에서 미리 정의해 둔 메뉴**라는 점입니다.  
> 
> 예를 들어 다음과 같이 매핑할 수 있습니다.
> 
> - 0: 제자리 대기  
> - 1: 위로 이동  
> - 2: 아래로 이동  
> - 3: 왼쪽 이동  
> - 4: 오른쪽 이동  
> - 5: 가장 가까운 적 공격  
> - 6: HP가 가장 낮은 적 공격  
> - 7: 범위 공격 스킬  
> - 8: 힐 스킬  
> - 9: 도발 스킬  
> - 10: 관통샷 스킬  
> - 11: 버프 스킬  
> 
> 환경 코드에서 `0~11`번에 어떤 행동을 할당할지는 **우리가 결정**하고,  
> 정책 네트워크는 이 12개 중에서 **각 행동을 선택할 확률**만 학습합니다.  
> 
> 즉, 네트워크 입장에서는 그저 **슬롯 0~11번**이 있을 뿐이고,  
> “7번이 범위 공격이다” 같은 의미는 환경 쪽 코드에서 정해지는 약속입니다.

---

## 슬라이드 8 – 보상 설계(1): 희소 + 밀집 보상

**키워드: 승패 보상 / 전투 행동 보상**

> 보상은 크게 세 층으로 구성했습니다.  
> 
> 첫 번째는 **희소 보상**으로, 게임의 최종 승패에 대한 보상입니다.  
> - 승리 시 +25  
> - 패배 시 -15  
> - 무승부 시 -10  
> 
> 두 번째는 **밀집 보상**으로, 전투 과정에서의 행동을 유도하기 위한 보상입니다.  
> - 적 처치 시 +15  
> - 적에게 준 데미지 1HP당 +0.5  
> - 적을 향해 접근하면 +0.3  
> - 제자리 대기 시 -0.5  
> - 전장에서 너무 멀리 떨어지면 거리 비례 패널티  
> 
> 이렇게 설계한 이유는, 승·패 보상만 주면  
> “우연히 이기는 경우를 기다리는” 식으로 학습이 매우 느려지기 때문입니다.  
> 밀집 보상을 통해 **공격적이고 적극적인 전투 행동**을 유도했습니다.

---

## 슬라이드 9 – 보상 설계(2): 역할별 + 협동 보상

**키워드: 역할 특화 보상 / 플레이어와 협동**

> 세 번째 층은 **역할별 보상과 협동 보상**입니다.  
> 
> - 탱커는 맞을수록 +0.15 어그로 보상을 받고, 죽으면 -2 페널티를 줘서  
>   “몸으로 맞되, 죽지는 않는” 플레이를 유도했습니다.  
> - 딜러는 준 데미지 1HP당 +0.3,  
> - 힐러는 힐량 1HP당 +0.15,  
> - 레인저는 일정 거리 이상에서 공격하면 +0.1,  
> - 서포터는 근처 아군 수에 비례한 버프 보상을 받습니다.  
> 
> 2단계 협동 학습에서는 여기에 **플레이어와의 거리 기반 보상**을 추가했습니다.  
> - 플레이어와 3칸 이내 거리를 유지하면 +2.5  
> - 5칸 이상 멀어지면 -1.5  
> - 플레이어가 공격하는 적을 같이 공격하면 +15  
> - 플레이어 대신 탱킹하거나 도발로 보호하면 추가 보상  
> 
> 이런 보상 설계를 통해, 초보 플레이어일수록 NPC가 더 가까이 붙어서 보호하고,  
> 숙련 플레이어일수록 뒤에서 시너지를 맞추는 **적응형 협동 스타일**을 유도했습니다.

---

## 슬라이드 10 – 알고리즘 개요: PPO + Actor-Critic 직관

**키워드: Actor / Critic / 천천히 정책 바꾸기**

> 이제 학습 알고리즘인 **PPO와 Actor-Critic 구조**를 설명드리겠습니다.  
> 
> 먼저 Actor-Critic은 쉽게 말하면,  
> **행동을 고르는 친구(Actor)**와 **상황을 평가하는 친구(Critic)**가  
> 하나의 신경망 안에 같이 들어 있는 구조입니다.  
> 
> - Actor는 상태 \(s_t\)를 보고,  
>   12개 행동에 대한 확률 \(\pi_\theta(a\mid s_t)\)를 출력합니다.  
> - Critic은 같은 상태를 보고,  
>   이 상태가 전체적으로 얼마나 좋은지 \(V_\theta(s_t)\)라는 숫자로 평가합니다.  
> 
> 저희 네트워크는 229차원 상태를 입력으로 받아  
> 256 유닛 두 개의 **공유 은닉층**을 거친 뒤,  
> 한쪽으로는 12차원 Softmax를 출력하는 Actor 헤드,  
> 다른 한쪽으로는 스칼라 값을 출력하는 Critic 헤드로 분기되는 구조입니다.  
> 
> PPO는 한 번에 정책이 너무 크게 바뀌면 학습이 불안정해지는 문제를 막기 위해,  
> **정책 변화 폭을 제한하면서 천천히 좋아지게 만드는 알고리즘**이라고 이해하시면 됩니다.

---

## 슬라이드 11 – 수식 흐름(1): Critic – δ\_t와 GAE 어드밴티지

> 먼저 Critic 쪽 수식부터 보겠습니다.  
> 
> Critic은 “지금 이 상태가 좋은지 나쁜지”를 예측하는 역할을 합니다.  
> 한 스텝에서 실제로 벌어진 일과, 내가 예상한 것의 차이를  
> **TD 오차 \(\delta_t\)** 라고 합니다.
> 
> \[
\delta_t = r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t)
\]
> 
> - \(r_t\): 이번 스텝에서 받은 보상  
> - \(\gamma\): 할인율, 미래 보상에 얼마나 가중치를 줄지 결정하는 값  
> - \(V_\theta(s_t)\): 지금 상태를 얼마나 좋은 상태라고 예상했는지  
> 
> \(\delta_t > 0\)이면 “생각보다 상황이 더 좋아졌다”는 뜻이고,  
> \(\delta_t < 0\)이면 “생각보다 안 좋아졌다”는 뜻입니다.  
> 이 차이를 줄이도록 Critic을 학습시키는 것이 **가치함수 학습**입니다.  
> 
> 여기서 한 걸음 더 나가서, Actor를 업데이트하기 위해  
> **이번 행동이 평균보다 얼마나 좋았는지를 나타내는 값**,  
> 즉 **어드밴티지 \( \hat{A}_t \)**가 필요합니다.  
> 저희는 GAE(Generalized Advantage Estimation)를 사용했고,  
> 개념적으로는 여러 스텝의 \(\delta\)를 \((\gamma\lambda)^l\)로 할인해서 합친 값입니다.
> 
> \[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\]
> 
> 쉽게 말하면, “이번 행동 이후에 얼마나 좋은 일들이 이어졌는지”를  
> 감가율 \(\gamma\)와 \(\lambda\)로 조절해서 보는 값이라고 이해하시면 됩니다.  
> \(\hat{A}_t > 0\)이면 잘한 행동, \(\hat{A}_t < 0\)이면 덜 좋은 행동입니다.

---

## 슬라이드 12 – 수식 흐름(2): PPO 클리핑 목적함수

> 이제 PPO의 핵심 수식인 **클리핑 목적함수**를 보겠습니다.
> 
> \[
L_{\text{CLIP}}(\theta)
=
\mathbb{E}_t\Big[
\min\big(
r_t(\theta)\,\hat{A}_t,\;
\text{clip}\big(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,\hat{A}_t
\big)
\Big]
\]
> 
> 여기서 \( r_t(\theta) \)는 **새 정책과 옛 정책의 확률 비율**입니다.
> 
> \[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
\]
> 
> - 분자는 **현재 정책**이 상태 \(s_t\)에서 행동 \(a_t\)를 선택할 확률,  
> - 분모는 **옛 정책**이 같은 행동을 선택할 확률입니다.  
> 
> 예를 들어, 예전에는 어떤 상황에서 힐을 40% 정도 쓰던 정책이  
> 지금은 80% 확률로 힐을 쓰려고 하면,  
> \(r_t = 0.8 / 0.4 = 2\)가 됩니다.  
> 즉, “예전보다 2배 더 자주 하려 한다”는 뜻입니다.  
> 
> 여기서 PPO는,  
> \(\hat{A}_t\)가 양수라서 잘한 행동이라고 하더라도  
> 한 번 업데이트에 이런 변화가 너무 크면 **불안정해질 수 있으니**,  
> 
> \[
\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)
\]
> 
> 을 사용해서 \(r_t\)를 0.8~1.2 범위로 잘라줍니다.  
> 그리고 원래 값 \(r_t \hat{A}_t\)와, 잘린 값 \(\text{clip}(r_t)\hat{A}_t\) 중에서  
> 더 **작은 쪽(min)** 을 택해서 업데이트합니다.  
> 
> 직관적으로는
> 
> > “잘한 행동이더라도, 한 번에 행동 확률을 2배, 3배로 확 올리지 말고,  
> > 20% 정도 선에서 **천천히** 올려라.”  
> 
> 라는 안정화 장치라고 보시면 됩니다.

---

## 슬라이드 13 – 최종 Loss: Actor + Critic + Entropy

> 최종적으로는 Actor, Critic, 그리고 탐험을 위한 엔트로피 항을 합쳐서  
> 다음과 같은 형태의 Loss를 최소화합니다.
> 
> \[
L_{\text{total}}(\theta)
=
L_{\text{actor}}(\theta)
+ c_v\,L_{\text{critic}}(\theta)
- c_{\text{ent}}\,\mathbb{E}_t\big[ H(\pi_\theta(\cdot \mid s_t)) \big]
\]
> 
> - \(L_{\text{actor}}\): 방금 설명드린 PPO 클리핑 목적함수에 마이너스를 붙인 값  
> - \(L_{\text{critic}}\): 상태 가치 예측 오차의 제곱합(MSE)  
> - \(H(\pi)\): 정책의 엔트로피, 즉 얼마나 랜덤하게 행동하는지  
> - \(c_v\): 가치함수 손실 가중치(0.5)  
> - \(c_{\text{ent}}\): 엔트로피 가중치(0.01)  
> 
> 이렇게 구성하면,  
> - Actor는 잘한 행동의 확률을 조금씩 올리고, 못한 행동은 줄이면서도,  
> - Critic은 상태의 가치를 더 정확하게 예측하도록 학습되고,  
> - 엔트로피 항 덕분에 정책이 **너무 빨리 한 행동에만 고착되지 않고**,  
>   일정 수준 탐험을 유지할 수 있습니다.

---

## 슬라이드 14 – 네트워크 구조(229→256→256, Softmax 12)

> 네트워크 구조를 정리하면 다음과 같습니다.
> 
> - 입력: 229차원 상태 벡터  
> - 공유 은닉층 1: Linear(229→256) + ReLU  
> - 공유 은닉층 2: Linear(256→256) + ReLU  
> - Actor Head: Linear(256→128) → Softmax(12)  
> - Critic Head: Linear(256→128) → Linear(1)  
> 
> 229→256→256은, “상황 설명 229개 숫자”를  
> 256칸짜리 메모장 두 번을 거쳐서  
> 더 추상적인 전투 상황 특징으로 바꿔주는 과정이라고 이해하실 수 있습니다.  
> 이렇게 추출된 256차원 피처를, Actor와 Critic이 같이 공유해서 사용합니다.  
> 
> Actor 쪽 마지막에는 12차원 Softmax를 사용해서  
> 앞에서 정의한 12개의 이산 행동에 대한 확률 분포를 출력합니다.  
> 이 확률을 기반으로 카테고리 분포에서 샘플링해서 실제 행동을 선택합니다.

---

## 슬라이드 15 – 하이퍼파라미터 설정

> PPO 하이퍼파라미터는 다음과 같이 설정했습니다.
> 
> - 학습률: \(3\times 10^{-4}\)  
> - 할인율 \(\gamma\): 0.99  
> - GAE \(\lambda\): 0.95  
> - 클리핑 \(\epsilon\): 0.2  
> - 엔트로피 계수: 0.01  
> - Value 계수: 0.5  
> - 배치 사이즈: 256  
> - 업데이트당 에폭: 4  
> - 옵티마이저: Adam  
> 
> \(\gamma=0.99, \lambda=0.95\)는 에피소드 후반의 승패까지 고려하면서도  
> 어드밴티지 추정이 너무 노이즈가 커지지 않도록 하는 조합이고,  
> \(\epsilon=0.2\)는 정책 변화 폭을 20% 수준으로 제한해서 안정성을 확보하기 위한 값입니다.  
> 엔트로피 계수 0.01은 탐험을 유지하기 위한 보너스,  
> Value 계수 0.5는 정책과 가치함수 학습의 비중을 맞추기 위한 계수입니다.  
> 여러 조합을 실험해본 결과, 이 값들이 가장 안정적인 학습 곡선을 보여 주었습니다.

---

## 슬라이드 16 – 실험 환경 및 구현 구조

> 실험은 i7-14700F, RTX 4060 Ti, 32GB RAM, Windows 11 환경에서 진행했습니다.  
> 소프트웨어는 Python 3.10, PyTorch 2.0, Unity 2022.3 LTS를 사용했고,  
> Python과 Unity는 UDP/TCP 소켓을 통해 통신하는 구조입니다.  
> 
> Python 쪽은 2D 격자 환경과 PPO 학습, 추론을 담당하고,  
> Unity 쪽은 3D 시각화와 VFX, 카메라 연출을 담당합니다.  
> 이런 구조 덕분에, PyTorch 모델을 ONNX로 변환하지 않고도  
> **약 1ms 수준의 지연으로 실시간 제어가 가능**했고,  
> 나중에 다른 엔진으로 포팅할 때도 Python 서버만 유지하면 되도록  
> 구조적인 유연성을 확보했습니다.

---

## 슬라이드 17 – 1단계 Self-Play 결과

> 1단계 Self-Play에서는 총 12,000 에피소드, 약 1,993만 스텝을 학습했습니다.  
> 평균 FPS는 약 900 정도였고, 전체 학습 시간은 약 6.1시간이 소요되었습니다.  
> 
> 초기에는 한 팀의 승률이 90% 이상으로 치우친 상태였지만,  
> 학습이 진행되면서 점점 균형을 찾아가고,  
> 최종적으로는 양 팀 승률이 약 50.2 대 49.4로  
> **거의 50:50에 가까운 균형 상태**에 도달했습니다.  
> 
> 1단계의 목표는 특정 팀이 상대를 압도하는 AI가 아니라,  
> 어느 쪽에 붙여도 공정하게 싸울 수 있는 **균형 잡힌 전투 AI**를 만드는 것이었기 때문에,  
> 이 결과를 기준으로 체크포인트들을 숙련도별 플레이어 모델로 사용했습니다.

---

## 슬라이드 18 – 2단계 협동 학습 결과

> 2단계 협동 학습에서는 총 12,750 에피소드를 학습했습니다.  
> 초기에는 평균 보상이 약 -1297 수준으로, 협동이 거의 이루어지지 않는 상태였지만,  
> 학습이 진행되면서 평균 보상이 플러스로 전환되었고,  
> 최종적으로는 약 +1700 이상까지 상승하여  
> **보상 증가 폭이 약 3,000 이상**이 되었습니다.  
> 
> 승률도 최종적으로 약 47:52 수준으로 균형에 가까운 값을 보였고,  
> 특히 **평균 탱커-플레이어 거리(avg_tank_dist)** 가  
> 초기 10.0 수준에서 최종 2.91까지 감소한 것이 중요합니다.  
> 
> 이는 협동 NPC가 플레이어 탱커 주변에서 움직이면서  
> 플레이어를 따라붙고 보호하는 행동을 학습했다는 정량적 근거로 볼 수 있습니다.

---

## 슬라이드 19 – Unity 3D 데모 및 행동 패턴 관찰

> Unity 3D 데모에서는, 학습된 정책을 그대로 사용해서  
> 실제 3D 전장에서 협동 NPC의 행동을 시각적으로 확인할 수 있습니다.  
> 
> 예를 들어 플레이어가 딜러를 선택했을 때,  
> - 탱커는 플레이어 전방에서 적의 공격을 대신 받아주고,  
> - 힐러는 플레이어 근처 후방에서 체력이 낮은 아군을 우선적으로 치유하며,  
> - 레인저는 벽 지형을 활용해 거리를 유지하면서 사격하고,  
> - 서포터는 아군이 많이 모여 있는 지점을 중심으로 버프를 제공합니다.  
> 
> 이런 패턴은 별도의 규칙을 짜지 않고,  
> 보상 설계와 PPO 학습만으로 자연스럽게 형성된 결과입니다.

---

## 슬라이드 20 – 한계점 및 향후 연구

> 한계점으로는, 먼저 환경이 20×20 격자이기 때문에  
> 실제 MMORPG처럼 복잡한 3D 연속 공간을 완전히 반영하지 못한다는 점이 있습니다.  
> 그래서 이번 결과를 바로 상용 게임에 적용하기에는 일반화 한계가 있습니다.  
> 
> 또한, 셀프플레이 체크포인트를 초보·중급·숙련 플레이어 모델에 대응시킨 부분은  
> “학습이 진행될수록 전투력이 높아진다”는 가정에 기반한 간접적인 정의이기 때문에,  
> 실제 인간 플레이어의 숙련도 분포를 얼마나 잘 반영하는지는 추가 검증이 필요합니다.  
> 
> 마지막으로, 현재 평가는 모두 시뮬레이션 지표 기반이라,  
> 실제 플레이어 경험이 얼마나 좋아지는지는 UX 실험이 부족한 상태입니다.  
> 
> 향후 연구로는  
> - 3D 연속 공간 및 실제 게임 엔진에서의 적용,  
> - 실제 유저 데이터를 이용한 숙련도 모델 보정,  
> - A/B 테스트와 설문을 통한 UX 평가  
> 를 계획하고 있습니다.

---

## 슬라이드 21 – 결론

> 정리하면, 저희는  
> 
> 1. 셀프플레이와 협동 학습으로 구성된 **2단계 강화학습 파이프라인**을 통해,  
>    별도의 유저 데이터 수집 없이도 숙련도별 플레이어 모델과 협동 NPC를 함께 학습하는 방법을 제안했고,  
> 2. 1단계에서 균형 잡힌 전투 AI를, 2단계에서 플레이어를 따라붙고 보호하는 협동 행동을  
>    정량적 지표와 3D 데모로 확인했습니다.  
> 3. Python–Unity 분리 구조와 UDP 미러링을 사용해,  
>    실시간 데모가 가능한 **실용적인 시스템 수준 구현**까지 완료했습니다.  
> 
> 즉, 단순히 사람을 이기는 AI가 아니라,  
> **플레이어의 숙련도에 맞춰 협동 방식을 조절하는 전투 NPC**를 강화학습으로 구현했다는 점에서  
> 게임 AI와 UX 관점 모두에 의미 있는 결과를 얻었다고 생각합니다.  
> 
> 발표 마치겠습니다. 감사합니다.
